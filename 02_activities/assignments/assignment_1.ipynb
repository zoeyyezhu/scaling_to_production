{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with parquet files\n",
    "\n",
    "## Objective\n",
    "\n",
    "+ In this assignment, we will use the data downloaded with the module `data_manager` to create features.\n",
    "\n",
    "(11 pts total)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "+ This notebook assumes that price data is available to you in the environment variable `PRICE_DATA`. If you have not done so, then execute the notebook `production_2_data_engineering.ipynb` to create this data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the environment variables using dotenv. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../05_src/data/prices/\n"
     ]
    }
   ],
   "source": [
    "# Write your code below.\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the PRICE_DATA environment variable\n",
    "PRICE_DATA = os.getenv('PRICE_DATA')\n",
    "print(PRICE_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "dask.config.set({'dataframe.query-planning': True})\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the environment variable `PRICE_DATA`.\n",
    "+ Use [glob](https://docs.python.org/3/library/glob.html) to find the path of all parquet files in the directory `PRICE_DATA`.\n",
    "\n",
    "(1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../05_src/data/prices/'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRICE_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import glob as glob_module\n",
    "\n",
    "# Write your code below.\n",
    "PRICE_DATA = os.getenv(\"PRICE_DATA\")\n",
    "parquet_files  = glob(os.path.join(PRICE_DATA, '*/*/*.parquet'))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each ticker and using Dask, do the following:\n",
    "\n",
    "+ Add lags for variables Close and Adj_Close.\n",
    "+ Add returns based on Adjusted Close:\n",
    "    \n",
    "    - `returns`: (Adj Close / Adj Close_lag) - 1\n",
    "\n",
    "+ Add the following range: \n",
    "\n",
    "    - `hi_lo_range`: this is the day's High minus Low.\n",
    "\n",
    "+ Assign the result to `dd_feat`.\n",
    "\n",
    "(4 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask-expr 0.4.1 requires dask==2024.2.0, but you have dask 2024.7.0 which is incompatible.\n",
      "distributed 2024.2.0 requires dask==2024.2.0, but you have dask 2024.7.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask in c:\\users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages (2024.2.0)\n",
      "Collecting dask\n",
      "  Downloading dask-2024.7.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: click>=8.1 in c:\\users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages (from dask) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in c:\\users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages (from dask) (3.0.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages (from dask) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages (from dask) (24.0)\n",
      "Requirement already satisfied: partd>=1.4.0 in c:\\users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages (from dask) (1.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages (from dask) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages (from dask) (0.12.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in c:\\users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages (from dask) (7.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages (from click>=8.1->dask) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages (from importlib-metadata>=4.13.0->dask) (3.17.0)\n",
      "Requirement already satisfied: locket in c:\\users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages (from partd>=1.4.0->dask) (1.0.0)\n",
      "Downloading dask-2024.7.0-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.2/1.2 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.5/1.2 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.8/1.2 MB 5.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.0/1.2 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 5.2 MB/s eta 0:00:00\n",
      "Installing collected packages: dask\n",
      "  Attempting uninstall: dask\n",
      "    Found existing installation: dask 2024.2.0\n",
      "    Uninstalling dask-2024.2.0:\n",
      "      Successfully uninstalled dask-2024.2.0\n",
      "Successfully installed dask-2024.7.0\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'sector',\n",
      "       'subsector', 'year'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yezhu\\AppData\\Local\\Temp\\ipykernel_53424\\1117178657.py:19: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "  ddf['Close_lag'] = ddf.groupby(['sector', 'subsector'])['Close'].shift(1)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AssignAlign' object has no attribute 'keys'\n\nThis often means that you are attempting to use an unsupported API function. Current API coverage is documented here: https://github.com/dask-contrib/dask-expr/blob/main/README.md#api-coverage.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\dask_expr\\_core.py:424\u001b[0m, in \u001b[0;36mExpr.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AssignAlign' object has no attribute 'keys'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m ddf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhi_lo_range\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ddf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHigh\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m ddf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLow\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Persist the result to avoid recomputing\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m dd_feat \u001b[38;5;241m=\u001b[39m \u001b[43mddf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Display the first few rows of the resulting DataFrame to verify the transformations\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(dd_feat\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\dask_expr\\_collection.py:406\u001b[0m, in \u001b[0;36mFrameBase.persist\u001b[1;34m(self, fuse, **kwargs)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpersist\u001b[39m(\u001b[38;5;28mself\u001b[39m, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 406\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DaskMethodsMixin\u001b[38;5;241m.\u001b[39mpersist(out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\dask_expr\\_collection.py:437\u001b[0m, in \u001b[0;36mFrameBase.optimize\u001b[1;34m(self, fuse)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, fuse: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_collection(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuse\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\dask_expr\\_expr.py:92\u001b[0m, in \u001b[0;36mExpr.optimize\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m optimize(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\dask_expr\\_expr.py:2760\u001b[0m, in \u001b[0;36moptimize\u001b[1;34m(expr, fuse)\u001b[0m\n\u001b[0;32m   2739\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"High level query optimization\u001b[39;00m\n\u001b[0;32m   2740\u001b[0m \n\u001b[0;32m   2741\u001b[0m \u001b[38;5;124;03mThis leverages three optimization passes:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2756\u001b[0m \u001b[38;5;124;03moptimize_blockwise_fusion\u001b[39;00m\n\u001b[0;32m   2757\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2759\u001b[0m \u001b[38;5;66;03m# Simplify\u001b[39;00m\n\u001b[1;32m-> 2760\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimplify\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2762\u001b[0m \u001b[38;5;66;03m# Manipulate Expression to make it more efficient\u001b[39;00m\n\u001b[0;32m   2763\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mrewrite(kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtune\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\dask_expr\\_core.py:344\u001b[0m, in \u001b[0;36mExpr.simplify\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     dependents \u001b[38;5;241m=\u001b[39m collect_dependents(expr)\n\u001b[1;32m--> 344\u001b[0m     new \u001b[38;5;241m=\u001b[39m \u001b[43mexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimplify_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdependents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdependents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimplified\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m==\u001b[39m expr\u001b[38;5;241m.\u001b[39m_name:\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\dask_expr\\_core.py:323\u001b[0m, in \u001b[0;36mExpr.simplify_once\u001b[1;34m(self, dependents, simplified)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(operand, Expr):\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;66;03m# Bandaid for now, waiting for Singleton\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     dependents[operand\u001b[38;5;241m.\u001b[39m_name]\u001b[38;5;241m.\u001b[39mappend(weakref\u001b[38;5;241m.\u001b[39mref(expr))\n\u001b[1;32m--> 323\u001b[0m     new \u001b[38;5;241m=\u001b[39m \u001b[43moperand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimplify_once\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdependents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdependents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimplified\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimplified\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m     simplified[operand\u001b[38;5;241m.\u001b[39m_name] \u001b[38;5;241m=\u001b[39m new\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m!=\u001b[39m operand\u001b[38;5;241m.\u001b[39m_name:\n",
      "File \u001b[1;32mc:\\Users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\dask_expr\\_core.py:323\u001b[0m, in \u001b[0;36mExpr.simplify_once\u001b[1;34m(self, dependents, simplified)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(operand, Expr):\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;66;03m# Bandaid for now, waiting for Singleton\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     dependents[operand\u001b[38;5;241m.\u001b[39m_name]\u001b[38;5;241m.\u001b[39mappend(weakref\u001b[38;5;241m.\u001b[39mref(expr))\n\u001b[1;32m--> 323\u001b[0m     new \u001b[38;5;241m=\u001b[39m \u001b[43moperand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimplify_once\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdependents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdependents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimplified\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimplified\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m     simplified[operand\u001b[38;5;241m.\u001b[39m_name] \u001b[38;5;241m=\u001b[39m new\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m!=\u001b[39m operand\u001b[38;5;241m.\u001b[39m_name:\n",
      "    \u001b[1;31m[... skipping similar frames: Expr.simplify_once at line 323 (5 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\dask_expr\\_core.py:323\u001b[0m, in \u001b[0;36mExpr.simplify_once\u001b[1;34m(self, dependents, simplified)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(operand, Expr):\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;66;03m# Bandaid for now, waiting for Singleton\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     dependents[operand\u001b[38;5;241m.\u001b[39m_name]\u001b[38;5;241m.\u001b[39mappend(weakref\u001b[38;5;241m.\u001b[39mref(expr))\n\u001b[1;32m--> 323\u001b[0m     new \u001b[38;5;241m=\u001b[39m \u001b[43moperand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimplify_once\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdependents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdependents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimplified\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimplified\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m     simplified[operand\u001b[38;5;241m.\u001b[39m_name] \u001b[38;5;241m=\u001b[39m new\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m!=\u001b[39m operand\u001b[38;5;241m.\u001b[39m_name:\n",
      "File \u001b[1;32mc:\\Users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\dask_expr\\_core.py:306\u001b[0m, in \u001b[0;36mExpr.simplify_once\u001b[1;34m(self, dependents, simplified)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# Allow children to simplify their parents\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m expr\u001b[38;5;241m.\u001b[39mdependencies():\n\u001b[1;32m--> 306\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simplify_up\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdependents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m         out \u001b[38;5;241m=\u001b[39m expr\n",
      "File \u001b[1;32mc:\\Users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\dask_expr\\_expr.py:3203\u001b[0m, in \u001b[0;36mAssignAlign._simplify_up\u001b[1;34m(self, parent, dependents)\u001b[0m\n\u001b[0;32m   3200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(columns, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m   3201\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [columns]\n\u001b[1;32m-> 3203\u001b[0m cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m)\n\u001b[0;32m   3204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cols \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;241m.\u001b[39mcolumns):\n\u001b[0;32m   3205\u001b[0m     \u001b[38;5;66;03m# Protect against pushing the same projection twice\u001b[39;00m\n\u001b[0;32m   3206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yezhu\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\dask_expr\\_core.py:445\u001b[0m, in \u001b[0;36mExpr.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[0;32m    444\u001b[0m link \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/dask-contrib/dask-expr/blob/main/README.md#api-coverage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 445\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis often means that you are attempting to use an unsupported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI function. Current API coverage is documented here: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlink\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    449\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AssignAlign' object has no attribute 'keys'\n\nThis often means that you are attempting to use an unsupported API function. Current API coverage is documented here: https://github.com/dask-contrib/dask-expr/blob/main/README.md#api-coverage."
     ]
    }
   ],
   "source": [
    "# Write your code below.\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from glob import glob\n",
    "import dask.dataframe as dd\n",
    "# Read all parquet files into a Dask DataFrame\n",
    "ddf = dd.read_parquet(parquet_files)\n",
    "\n",
    "# Display the columns to confirm the structure\n",
    "print(ddf.columns)\n",
    "\n",
    "# Ensure 'Date' is in datetime format if not already\n",
    "ddf['Date'] = dd.to_datetime(ddf['Date'])\n",
    "\n",
    "# Sort by sector, subsector, and Date to ensure the correct order for lag calculations\n",
    "ddf = ddf.sort_values(by=['sector', 'subsector', 'Date'])\n",
    "\n",
    "# Add lagged features for Close\n",
    "ddf['Close_lag'] = ddf.groupby(['sector', 'subsector'])['Close'].shift(1)\n",
    "\n",
    "# Calculate returns based on Close\n",
    "ddf['returns'] = (ddf['Close'] / ddf['Close_lag']) - 1\n",
    "\n",
    "# Calculate the day's high-low range\n",
    "ddf['hi_lo_range'] = ddf['High'] - ddf['Low']\n",
    "\n",
    "# Persist the result to avoid recomputing\n",
    "dd_feat = ddf.persist()\n",
    "\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame to verify the transformations\n",
    "print(dd_feat.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Convert the Dask data frame to a pandas data frame. \n",
    "+ Add a rolling average return calculation with a window of 10 days.\n",
    "+ *Tip*: Consider using `.rolling(10).mean()`.\n",
    "\n",
    "(3 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dd_feat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Write your code below.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Convert the Dask DataFrame to a Pandas DataFrame\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m pd_df \u001b[38;5;241m=\u001b[39m \u001b[43mdd_feat\u001b[49m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Add a rolling average return calculation with a window of 10 days\u001b[39;00m\n\u001b[0;32m      7\u001b[0m pd_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrolling_avg_return\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd_df\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msector\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubsector\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mrolling(\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mmean())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dd_feat' is not defined"
     ]
    }
   ],
   "source": [
    "# Write your code below.\n",
    "\n",
    "# Convert the Dask DataFrame to a Pandas DataFrame\n",
    "pd_df = dd_feat.compute()\n",
    "\n",
    "# Add a rolling average return calculation with a window of 10 days\n",
    "pd_df['rolling_avg_return'] = pd_df.groupby(['sector', 'subsector'])['returns'].transform(lambda x: x.rolling(10).mean())\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame to verify the transformations\n",
    "print(pd_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please comment:\n",
    "\n",
    "+ Was it necessary to convert to pandas to calculate the moving average return?\n",
    "+ Would it have been better to do it in Dask? Why?\n",
    "\n",
    "(1 pt)\n",
    "It's not necessary to convert to Pandas to calculate the moving average return. yes,  Dask, you can avoid loading the entire dataset into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "|Criteria|Complete|Incomplete|\n",
    "|---------------------|----|----|\n",
    "|Calculations         |Calculations were done correctly.|Calculations were not done correctly.|\n",
    "|Explanation of answer|Answer was concise and explained the learner's reasoning in depth.|Answer was not concise and did not explained the learner's reasoning in depth.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "🚨 **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** 🚨 for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-1`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
